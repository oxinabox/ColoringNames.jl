{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using TensorFlow\n",
    "using Distributions\n",
    "using MLDataUtils\n",
    "using SwiftObjectStores\n",
    "using Iterators\n",
    "using ColoringNames\n",
    "using MappedArrays\n",
    "using MLLabelUtils\n",
    "using StaticArrays\n",
    "using Base.Test\n",
    "\n",
    "Base.one{S<:AbstractString}(::Type{S}) = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1523108×4 Array{Any,2}:\n",
       " \"acid green\"  0.344113  0.855319  0.921569\n",
       " \"acid green\"  0.281905  0.744681  0.921569\n",
       " \"acid green\"  0.347551  0.921397  0.898039\n",
       " \"acid green\"  0.311209  0.900398  0.984314\n",
       " \"acid green\"  0.312085  0.992095  0.992157\n",
       " \"acid green\"  0.22766   0.987395  0.933333\n",
       " \"acid green\"  0.288462  0.466368  0.87451 \n",
       " \"acid green\"  0.276936  0.804878  0.964706\n",
       " \"acid green\"  0.325359  0.95      0.862745\n",
       " \"acid green\"  0.385201  0.971774  0.972549\n",
       " \"acid green\"  0.212885  0.955823  0.976471\n",
       " \"acid green\"  0.346405  0.6       1.0     \n",
       " \"acid green\"  0.238397  0.42246   0.733333\n",
       " ⋮                                         \n",
       " \"yuck\"        0.1621    0.421965  0.678431\n",
       " \"yuck\"        0.153333  0.722543  0.678431\n",
       " \"yuck\"        0.201754  0.256757  0.580392\n",
       " \"yuck\"        0.264463  0.620513  0.764706\n",
       " \"yuck\"        0.144608  0.790698  0.67451 \n",
       " \"yuck\"        0.178125  0.97561   0.643137\n",
       " \"yuck\"        0.166667  0.350962  0.815686\n",
       " \"yuck\"        0.174603  0.640244  0.643137\n",
       " \"yuck\"        0.587607  0.397959  0.768627\n",
       " \"yuck\"        0.774242  0.973451  0.443137\n",
       " \"yuck\"        0.376894  0.768559  0.898039\n",
       " \"yuck\"        0.179104  0.41358   0.635294"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serv=SwiftService()\n",
    "train_raw = get_file(serv, \"color\", \"monroe/train.csv\") do fh\n",
    "    readdlm(fh,'\\t')\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::tokenize) (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = morpheme_tokenizer(\"../data/replacement_rules.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000×3 Array{Float64,2}:\n",
       " 0.539141  0.564103  0.917647\n",
       " 0.314928  0.831633  0.768627\n",
       " 0.22619   0.375     0.439216\n",
       " 0.243842  0.890351  0.894118\n",
       " 0.572304  0.559671  0.952941\n",
       " 0.677011  0.884146  0.643137\n",
       " 0.830128  0.327044  0.623529\n",
       " 0.652344  0.549356  0.913725\n",
       " 0.943452  0.823529  0.266667\n",
       " 0.948649  0.731225  0.992157\n",
       " 0.450549  0.654676  0.545098\n",
       " 0.422101  0.973545  0.741176\n",
       " 0.674954  0.780172  0.909804\n",
       " ⋮                           \n",
       " 0.732906  0.995745  0.921569\n",
       " 0.928     0.748503  0.654902\n",
       " 0.295198  0.967213  0.956863\n",
       " 0.235727  0.79386   0.894118\n",
       " 0.690476  0.573276  0.909804\n",
       " 0.238215  0.846154  0.917647\n",
       " 0.947917  0.898876  0.698039\n",
       " 0.387067  0.753086  0.952941\n",
       " 0.18107   0.707424  0.898039\n",
       " 0.204981  0.386667  0.882353\n",
       " 0.728797  0.698347  0.94902 \n",
       " 0.382456  0.88785   0.419608"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_inds = shuffle(1:size(train_raw, 1))[1:100_000]\n",
    "\n",
    "labels = convert(Vector{String}, train_raw[keep_inds,1]); #TODO use all data\n",
    "hsv_data = convert(Matrix{Float64}, train_raw[keep_inds,2:end]); #TODO use all data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000-element Array{Array{Int64,1},1}:\n",
       " [1]      \n",
       " [2]      \n",
       " [3]      \n",
       " [4,2]    \n",
       " [5]      \n",
       " [1]      \n",
       " [6,7]    \n",
       " [1]      \n",
       " [8,7]    \n",
       " [9]      \n",
       " [10,2]   \n",
       " [2]      \n",
       " [1]      \n",
       " ⋮        \n",
       " [7]      \n",
       " [33]     \n",
       " [2]      \n",
       " [2]      \n",
       " [1]      \n",
       " [2,13,17]\n",
       " [8,9]    \n",
       " [2]      \n",
       " [17,18,2]\n",
       " [19]     \n",
       " [7]      \n",
       " [8,2]    "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenised_labels = tokenizer.(labels)\n",
    "all_tokens = reduce(union, tokenised_labels)\n",
    "encoding = labelenc(all_tokens)\n",
    "label_inds = map(x->label2ind.(x, Scalar(encoding)), tokenised_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mTest Summary: | \u001b[0m\u001b[1m\u001b[32mPass  \u001b[0m\u001b[1m\u001b[34mTotal\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "function Base.rpad{T}(xs::Vector{T}, n::Integer, p::T=zero(T))\n",
    "    sizehint!(xs, n)\n",
    "    while length(xs)<n\n",
    "        push!(xs, p)\n",
    "    end\n",
    "    xs\n",
    "end\n",
    "\n",
    "\"Creates a matrix where each column is one of the vectors from `xss`\"\n",
    "function rpad_to_matrix{T}(xss::Vector{Vector{T}}, n_rows::T=maximum(length.(xss)), p::T=zero(T))\n",
    "    n_cols = length(xss) \n",
    "    ret = fill(p, (n_rows, n_cols))\n",
    "    for (cc, xs) in enumerate(xss)\n",
    "        for (rr, x) in enumerate(xs)\n",
    "            @inbounds ret[rr, cc] = x\n",
    "        end\n",
    "    end\n",
    "    ret\n",
    "end\n",
    "\n",
    "@testset \"padding\" begin\n",
    "    @test rpad([1,2,3],5) == [1,2,3,0,0]\n",
    "    @test rpad([1,2,3],2) == [1,2,3]\n",
    "    @test rpad([1,2,3],5, 8 ) == [1,2,3,8,8]\n",
    "    \n",
    "    @test rpad_to_matrix([[1,2], [3],Int[],[2]])==[\n",
    "                                                    1 3 0 2\n",
    "                                                    2 0 0 0\n",
    "    ]\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×100000 Array{Int64,2}:\n",
       " 1  2  3  4  5  1  6  1  8  9  10  2  …  33  2  2  1   2  8  2  17  19  7  8\n",
       " 0  0  0  2  0  0  7  0  7  0   2  0      0  0  0  0  13  9  0  18   0  0  2\n",
       " 0  0  0  0  0  0  0  0  0  0   0  0      0  0  0  0  17  0  0   2   0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0   0  0      0  0  0  0   0  0  0   0   0  0  0\n",
       " 0  0  0  0  0  0  0  0  0  0   0  0      0  0  0  0   0  0  0   0   0  0  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_labels = rpad_to_matrix(label_inds, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(n_steps,batch_size) = size(padded_labels) = (5,100000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "\n",
    "n_input = 3 # HSV\n",
    "@show n_steps, batch_size = size(padded_labels)\n",
    "hidden_layer_size = 256\n",
    "n_classes = nlabel(encoding)+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = Session(Graph())\n",
    "X = placeholder(Float32, shape=[batch_size, n_input]; name=\"X_HSVs\")\n",
    "Term_obs_s = placeholder(Int32, shape=[n_steps, batch_size]; name=\"Term_obs_s\")\n",
    "\n",
    "\n",
    "Xs = fill(X, n_steps)\n",
    "\n",
    "\n",
    "Hs, states = nn.rnn(nn.rnn_cell.LSTMCell(hidden_layer_size), Xs; dtype=Float32)#, sequence_length=n_steps);\n",
    "\n",
    "variable_scope(\"fromColor\", initializer=Normal(0, .1)) do\n",
    "    \n",
    "    \n",
    "    global W1 = get_variable(\"weights1\", [hidden_layer_size, n_classes], Float32)\n",
    "    global B1 = get_variable(\"bias1\", [n_classes], Float32)\n",
    "\n",
    "    global Ls =  [H*W1+B1 for H in Hs]\n",
    "    \n",
    "    #global Ls =  [TensorFlow.matmul(H, W1; name=\"Ls$ii\") for (ii,H) in enumerate(Hs)]\n",
    "end;\n",
    "\n",
    "\n",
    "#LL = concat(3, expand_dims(.(Ls, Scalar(3))); name=\"Stack_Logits\") #Stack\n",
    "#LL = concat(2, [expand_dims(L, 2; name=\"Le$ii\") for (ii,L) in enumerate(Ls)]; name=\"Stack_Logits\") #Stack\n",
    "LL = concat(0, Ls; name=\"Stack_Logits\") #Stack\n",
    "\n",
    "TT = reshape(Term_obs_s, [n_steps*batch_size]; name=\"Stack_Term_obs_s\")\n",
    "\n",
    "Term_preds_onehots = nn.softmax(LL; name=\"Term_preds_onehots\")\n",
    "Term_preds_s = indmax(Term_preds_onehots, 2)-1\n",
    "\n",
    "Term_obs_onehots = one_hot(TT, n_classes)\n",
    "costs  = (reduce_sum(Term_obs_onehots.*log(Term_preds_onehots), reduction_indices=[1]))\n",
    "\n",
    "#costs = nn.sparse_softmax_cross_entropy_with_logits(LL, TT+1)\n",
    "cost = reduce_mean(-costs) #cross entropy\n",
    "\n",
    "\n",
    "\n",
    "optimizer = train.minimize(train.AdamOptimizer(learning_rate), cost)\n",
    "\n",
    "run(sess, initialize_all_variables())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global Term_preds_s_o\n",
    "global cost_o\n",
    "global optimizer_o\n",
    "for i in 1:1000\n",
    "    Term_preds_s_o,  cost_o, optimizer_o = run(sess, [Term_preds_s, cost, optimizer], Dict(X=>hsv_data, Term_obs_s=>padded_labels))\n",
    "    @show (i, cost_o)\n",
    "    @show (i, unique(Term_preds_s_o))\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nope"
     ]
    }
   ],
   "source": [
    "ind2label.(Term_preds_s_o[Term_preds_s_o.>0], Scalar(encoding)) |> print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind2label.(Term_preds_s_o, Scalar(encoding)) |> print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_shape.(x) = TensorFlow.ShapeInference.TensorShape[TensorShape[1524, 3],TensorShape[1524, 3],TensorShape[1524, 3],TensorShape[1524, 3],TensorShape[1524, 3]]\n"
     ]
    }
   ],
   "source": [
    "x = transpose(X, Int32.([1, 3, 2].-1)) # Permuting batch_size and n_steps. (the -1 is to use 0 based indexing)\n",
    "x = reshape(x, [n_steps*batch_size, n_input]) # Reshaping to (n_steps*batch_size, n_input)\n",
    "x = split(1, n_steps, x) # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "@show get_shape.(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: tokens not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: tokens not defined",
      "",
      " in (::##13#14)() at ./In[15]:8",
      " in #variable_scope#63(::Array{Any,1}, ::Function, ::##13#14, ::String) at /home/ubuntu/.julia/v0.5/TensorFlow/src/variable.jl:206",
      " in (::TensorFlow.#kw##variable_scope)(::Array{Any,1}, ::TensorFlow.#variable_scope, ::Function, ::String) at ./<missing>:0"
     ]
    }
   ],
   "source": [
    "sess = Session(Graph())\n",
    "\n",
    "embedding_dim = 16\n",
    "hidden_layer_size = 128\n",
    "variable_scope(\"ToColor\", initializer=Normal(0, .1)) do\n",
    "    global hsv_obs = placeholder(Float64)\n",
    "    global word_ids = placeholder(Int64)\n",
    "    global Ts = get_variable(\"TokenEmbeddings\",  [length(tokens), embedding_dim], Float32)\n",
    "    global X_embs = nn.embedding_lookup(Ts,  X_ids);\n",
    "    global SoWe = reduce_sum(X_embs; reduction_indices=1, keep_dims=true); #Sum up however many embeddings we have\n",
    "    \n",
    "    global W1 = get_variable(\"weights1\", [embedding_dim, hidden_layer_size], Float32)\n",
    "    global B1 = get_variable(\"bias1\", [hidden_layer_size,], Float32)    \n",
    "    \n",
    "    global Wo = get_variable(\"weightsout\", [hidden_layer_size, output_dim], Float32)\n",
    "    global Bo = get_variable(\"biasout\", [output_dim,], Float32)    \n",
    "    \n",
    "    global H = nn.sigmoid(SoWe*W1 + B1)\n",
    "    global Y = nn.tanh(H*Wo + Bo)\n",
    "    global Loss = reduce_sum((Y - Y_obs)^2)\n",
    "\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "1 method for generic function <b>sparse_softmax_cross_entropy_with_logits</b>:<ul><li> sparse_softmax_cross_entropy_with_logits(logits, labels; <i>name</i>) at <a href=\"https://github.com/malmaud/TensorFlow.jl/tree/1127fb5864b6513924f8c44cacc46102189fabc5/src/ops/nn.jl#L201\" target=\"_blank\">/home/ubuntu/.julia/v0.5/TensorFlow/src/ops/nn.jl:201</a></li> </ul>"
      ],
      "text/plain": [
       "# 1 method for generic function \"sparse_softmax_cross_entropy_with_logits\":\n",
       "sparse_softmax_cross_entropy_with_logits(logits, labels; name) at /home/ubuntu/.julia/v0.5/TensorFlow/src/ops/nn.jl:201"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(nn.sparse_softmax_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition draw(Any) in module Main at In[50]:11 overwritten at In[52]:11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1-element Array{Float64,1}:\n",
       " -230.249"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions\n",
    "using TensorFlow\n",
    "\n",
    "# Generate some synthetic data\n",
    "x = randn(100, 50)\n",
    "w = randn(50, 10)\n",
    "y_prob = exp(x*w)\n",
    "y_prob ./= sum(y_prob,2)\n",
    "\n",
    "function draw(probs)\n",
    "    y = zeros(size(probs))\n",
    "    for i in 1:size(probs, 1)\n",
    "        idx = rand(Categorical(probs[i, :]))\n",
    "        y[i, idx] = 1\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "\n",
    "y = draw(y_prob)\n",
    "\n",
    "# Build the model\n",
    "sess = Session(Graph())\n",
    "X = placeholder(Float64)\n",
    "Y_obs = placeholder(Float64)\n",
    "Y_obs_lbl = indmax(Y_obs, 2)\n",
    "variable_scope(\"logisitic_model\", initializer=Normal(0, .001)) do\n",
    "    global W = get_variable(\"weights\", [50, 10], Float64)\n",
    "    global B = get_variable(\"bias\", [10], Float64)\n",
    "end\n",
    "\n",
    "L = X*W + B\n",
    "Y=nn.softmax(L)\n",
    "#costs = log(Y).*Y_obs #Dense (Orginal) way\n",
    "costs = nn.sparse_softmax_cross_entropy_with_logits(L, Y_obs_lbl+1) #sparse way\n",
    "Loss = -reduce_sum(costs)\n",
    "optimizer = train.AdamOptimizer()\n",
    "minimize_op = train.minimize(optimizer, Loss)\n",
    "saver = train.Saver()\n",
    "# Run training\n",
    "run(sess, initialize_all_variables())\n",
    "cur_loss, = run(sess, [Loss], Dict(X=>x, Y_obs=>y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "4 methods for generic function <b>indmax</b>:<ul><li> indmax(f::<b>Array{Base.Pkg.Resolve.MaxSum.FieldValues.FieldValue,1}</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/6a1e339ac1ca24b0d00c1721a1833fb39106c559/base/pkg/resolve/fieldvalue.jl#L74\" target=\"_blank\">pkg/resolve/fieldvalue.jl:74</a></li> <li> indmax(A::<b>SparseMatrixCSC</b>) at <a href=\"https://github.com/JuliaLang/julia/tree/6a1e339ac1ca24b0d00c1721a1833fb39106c559/base/sparse/sparsematrix.jl#L2073\" target=\"_blank\">sparse/sparsematrix.jl:2073</a></li> <li> indmax(n::<b>TensorFlow.AbstractTensor</b>, dim) at <a href=\"https://github.com/malmaud/TensorFlow.jl/tree/1127fb5864b6513924f8c44cacc46102189fabc5/src/ops/math.jl#L68\" target=\"_blank\">/home/ubuntu/.julia/v0.5/TensorFlow/src/ops/math.jl:68</a></li> <li> indmax(a) at <a href=\"https://github.com/JuliaLang/julia/tree/6a1e339ac1ca24b0d00c1721a1833fb39106c559/base/array.jl#L1281\" target=\"_blank\">array.jl:1281</a></li> </ul>"
      ],
      "text/plain": [
       "# 4 methods for generic function \"indmax\":\n",
       "indmax(f::Array{Base.Pkg.Resolve.MaxSum.FieldValues.FieldValue,1}) at pkg/resolve/fieldvalue.jl:74\n",
       "indmax(A::SparseMatrixCSC) at sparse/sparsematrix.jl:2073\n",
       "indmax(n::TensorFlow.AbstractTensor, dim) at /home/ubuntu/.julia/v0.5/TensorFlow/src/ops/math.jl:68\n",
       "indmax(a) at array.jl:1281"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(indmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1-pre",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
